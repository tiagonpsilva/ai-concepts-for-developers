# üß† Caso de Uso: Sistemas de Recomenda√ß√£o Din√¢micos

## üéØ Objetivo

Desenvolver um sistema de recomenda√ß√£o avan√ßado baseado em Reinforcement Learning (RL) que se adapte dinamicamente √†s prefer√™ncias do usu√°rio, maximizando o engajamento de longo prazo e proporcionando recomenda√ß√µes personalizadas que evoluem com o tempo.

## üîç Problema de Neg√≥cio

Os sistemas de recomenda√ß√£o tradicionais enfrentam desafios significativos:

- Foco excessivo em recompensa imediata (cliques) em detrimento de valor de longo prazo
- Dificuldade em equilibrar explora√ß√£o de novos interesses e explora√ß√£o de prefer√™ncias conhecidas
- "Filter bubbles" e recomenda√ß√µes homog√™neas que limitam descobertas
- Incapacidade de adaptar-se rapidamente √†s mudan√ßas de prefer√™ncia
- Falta de considera√ß√£o do contexto e estado atual do usu√°rio

O Reinforcement Learning oferece uma solu√ß√£o din√¢mica que pode otimizar simultaneamente objetivos de curto e longo prazo, explorando de maneira inteligente o espa√ßo de conte√∫do e adaptando-se √†s mudan√ßas de comportamento e interesses.

## üìä Modelagem do Problema

### Formula√ß√£o como MDP (Processo de Decis√£o de Markov)

```mermaid
graph TD
    A[Estado] --> A1[Hist√≥rico do Usu√°rio]
    A --> A2[Contexto Atual]
    A --> A3[Comportamento Recente]
    A --> A4[Feedback Expl√≠cito/Impl√≠cito]
    A --> A5[Hora/Localiza√ß√£o/Dispositivo]
    
    B[A√ß√µes] --> B1[Itens a Recomendar]
    B --> B2[Diversidade da Lista]
    B --> B3[Ordem de Apresenta√ß√£o]
    B --> B4[Momento da Recomenda√ß√£o]
    
    C[Recompensas] --> C1[Cliques/Visualiza√ß√µes]
    C --> C2[Tempo de Engajamento]
    C --> C3[Compras/Convers√µes]
    C --> C4[Reten√ß√£o de Longo Prazo]
    C --> C5[Feedback Expl√≠cito]
```

### Defini√ß√£o Formal

- **Estados (S)**: Representa√ß√£o do usu√°rio, seu hist√≥rico, contexto e comportamento
- **A√ß√µes (A)**: Conjunto de itens poss√≠veis para recomendar
- **Transi√ß√µes (P)**: Como as a√ß√µes afetam o estado do usu√°rio
- **Recompensas (R)**: Feedback imediato e m√©tricas de engajamento de longo prazo
- **Pol√≠tica (œÄ)**: Estrat√©gia para selecionar recomenda√ß√µes em cada estado

## üõ†Ô∏è Arquitetura do Sistema

```mermaid
graph TD
    A[Dados do Usu√°rio] --> B[Processamento de Features]
    B --> C[Estado do Usu√°rio]
    C --> D[Agente RL]
    
    E[Cat√°logo de Conte√∫do] --> F[Feature Extraction]
    F --> G[Representa√ß√£o de Itens]
    G --> D
    
    D --> H[Pol√≠tica de Recomenda√ß√£o]
    H --> I[Top-K Recomenda√ß√µes]
    I --> J[Interface do Usu√°rio]
    
    J --> K[Intera√ß√µes do Usu√°rio]
    K --> L[Feedback Loop]
    L --> M[C√°lculo de Recompensa]
    M --> D
```

### 1. Componentes do Sistema

#### Representa√ß√£o do Usu√°rio e Contexto

```python
class UserState:
    def __init__(self, user_id, history, context):
        # Informa√ß√µes b√°sicas do usu√°rio
        self.user_id = user_id
        self.demographics = get_user_demographics(user_id)
        
        # Hist√≥rico de intera√ß√µes
        self.short_term_history = history['short_term']  # √∫ltimas 20 intera√ß√µes
        self.long_term_history = history['long_term']    # padr√µes hist√≥ricos
        
        # Embeddings de prefer√™ncias
        self.item_embedding_avg = compute_avg_embedding(history['items'])
        self.category_preferences = compute_category_preferences(history)
        
        # Contexto atual
        self.time_of_day = context['time_of_day']
        self.day_of_week = context['day_of_week']
        self.device = context['device']
        self.location = context['location']
        
        # Estado da sess√£o
        self.session_length = context['session_length']
        self.last_interactions = context['last_interactions']
        
        # M√©tricas de engajamento
        self.avg_session_length = history['avg_session_length']
        self.return_rate = history['return_rate']
        self.conversion_rate = history['conversion_rate']
```

#### Representa√ß√£o de Itens

```python
class ItemLibrary:
    def __init__(self, catalog_items):
        self.items = {
            item_id: {
                'embedding': compute_item_embedding(item),
                'categories': item['categories'],
                'popularity': item['popularity'],
                'recency': item['recency'],
                'features': extract_features(item)
            } for item_id, item in catalog_items.items()
        }
        
        # Criar √≠ndices para busca eficiente
        self.category_index = build_category_index(self.items)
        self.embedding_index = build_ann_index(self.items)  # Approximate Nearest Neighbors
        
    def get_similar_items(self, item_id, n=10):
        """Retorna os N itens mais similares a um item espec√≠fico"""
        if item_id not in self.items:
            return []
        
        query_embedding = self.items[item_id]['embedding']
        similar_ids = self.embedding_index.get_nearest(query_embedding, n)
        return similar_ids
    
    def get_items_by_category(self, category, n=50):
        """Retorna at√© N itens de uma categoria espec√≠fica"""
        if category not in self.category_index:
            return []
        
        return list(self.category_index[category])[:n]
    
    def get_candidate_items(self, user_state, n=100):
        """Gera candidatos para recomenda√ß√£o baseados no estado do usu√°rio"""
        candidates = set()
        
        # Itens similares aos que o usu√°rio interagiu recentemente
        for item_id in user_state.short_term_history:
            similar = self.get_similar_items(item_id, n=10)
            candidates.update(similar)
        
        # Itens das categorias preferidas
        for category, score in user_state.category_preferences.items():
            if score > 0.2:  # Limiar de interesse
                category_items = self.get_items_by_category(category, n=25)
                candidates.update(category_items)
        
        # Remover itens que o usu√°rio j√° interagiu
        all_history = set(user_state.short_term_history + user_state.long_term_history)
        candidates = candidates - all_history
        
        # Limitar n√∫mero de candidatos
        return list(candidates)[:n]
```

## üíª Implementa√ß√£o com RL Profundo

### Algoritmo: Deep Q-Network (DQN) com Dueling Architecture

O DQN √© adequado para este caso por sua capacidade de lidar com espa√ßos de a√ß√£o grandes e discretos, al√©m de manter a estabilidade durante o treinamento.

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import random
from collections import deque

class DuelingDQNAgent:
    def __init__(
        self,
        state_dim,
        action_dim,
        learning_rate=0.001,
        gamma=0.95,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01,
        batch_size=64,
        memory_size=10000
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma  # fator de desconto
        self.epsilon = epsilon  # explora√ß√£o
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size
        
        # Mem√≥ria de experi√™ncia
        self.memory = deque(maxlen=memory_size)
        
        # Modelos Q
        self.model = self.build_dueling_dqn()
        self.target_model = self.build_dueling_dqn()
        self.update_target_model()
    
    def build_dueling_dqn(self):
        """Constr√≥i a arquitetura de rede dueling DQN"""
        # Camada de entrada
        input_layer = layers.Input(shape=(self.state_dim,))
        
        # Camadas compartilhadas
        x = layers.Dense(256, activation='relu')(input_layer)
        x = layers.Dense(256, activation='relu')(x)
        
        # Stream de Valor (V)
        value_stream = layers.Dense(128, activation='relu')(x)
        value = layers.Dense(1)(value_stream)
        
        # Stream de Vantagem (A)
        advantage_stream = layers.Dense(128, activation='relu')(x)
        advantage = layers.Dense(self.action_dim)(advantage_stream)
        
        # Combinar streams - Q(s,a) = V(s) + A(s,a) - mean(A(s,a))
        q_values = value + (advantage - tf.reduce_mean(advantage, axis=1, keepdims=True))
        
        model = keras.Model(inputs=input_layer, outputs=q_values)
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')
        
        return model
    
    def update_target_model(self):
        """Atualiza o modelo alvo com pesos do modelo principal"""
        self.target_model.set_weights(self.model.get_weights())
    
    def remember(self, state, action, reward, next_state, done):
        """Armazena experi√™ncia na mem√≥ria"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state, training=True):
        """Seleciona uma a√ß√£o usando a pol√≠tica epsilon-greedy"""
        if training and np.random.rand() < self.epsilon:
            # Explora√ß√£o - a√ß√£o aleat√≥ria
            return random.randrange(self.action_dim)
        
        # Explora√ß√£o - usar modelo para prever valores Q
        q_values = self.model.predict(state.reshape(1, -1), verbose=0)[0]
        return np.argmax(q_values)  # a√ß√£o com maior valor Q
    
    def replay(self):
        """Treina o modelo com experi√™ncias passadas (experience replay)"""
        if len(self.memory) < self.batch_size:
            return
        
        # Amostra de experi√™ncias
        minibatch = random.sample(self.memory, self.batch_size)
        
        for state, action, reward, next_state, done in minibatch:
            target = reward
            
            if not done:
                # Double DQN: usar modelo principal para selecionar a√ß√£o,
                # mas modelo alvo para avaliar
                actions = self.model.predict(next_state.reshape(1, -1), verbose=0)[0]
                action_idx = np.argmax(actions)
                target_q = self.target_model.predict(next_state.reshape(1, -1), verbose=0)[0]
                target += self.gamma * target_q[action_idx]
            
            # Atualizar valor Q para a a√ß√£o tomada
            target_f = self.model.predict(state.reshape(1, -1), verbose=0)
            target_f[0][action] = target
            
            # Treinar modelo
            self.model.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)
        
        # Decair epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save(self, file_path):
        """Salva o modelo em disco"""
        self.model.save(file_path)
    
    def load(self, file_path):
        """Carrega o modelo do disco"""
        self.model = keras.models.load_model(file_path)
        self.update_target_model()
```

### Classe RecommendationEnvironment para Treinamento

```python
class RecommendationEnvironment:
    def __init__(self, user_data, item_library):
        self.user_data = user_data
        self.item_library = item_library
        self.current_user = None
        self.current_state = None
        self.recommended_items = []
        self.step_count = 0
        self.max_steps = 20  # m√°ximo de recomenda√ß√µes por epis√≥dio
    
    def reset(self, user_id=None):
        """Reinicia o ambiente com um usu√°rio aleat√≥rio ou espec√≠fico"""
        if user_id is None:
            user_id = random.choice(list(self.user_data.keys()))
        
        self.current_user = user_id
        history = self.user_data[user_id]['history']
        context = self.user_data[user_id]['context']
        
        self.current_state = self.create_state_representation(user_id, history, context)
        self.recommended_items = []
        self.step_count = 0
        
        return self.current_state, {}
    
    def create_state_representation(self, user_id, history, context):
        """Cria representa√ß√£o vetorial do estado"""
        user_state = UserState(user_id, history, context)
        
        # Converter UserState para vetor
        state_vector = []
        
        # Adicionar caracter√≠sticas do usu√°rio
        state_vector.extend(user_state.item_embedding_avg)
        
        # One-hot encoding das categorias preferidas
        category_vector = [0] * len(self.item_library.category_index)
        for cat_idx, category in enumerate(self.item_library.category_index.keys()):
            if category in user_state.category_preferences:
                category_vector[cat_idx] = user_state.category_preferences[category]
        state_vector.extend(category_vector)
        
        # Adicionar contexto
        state_vector.extend([
            user_state.time_of_day / 24.0,  # normalizado
            user_state.day_of_week / 7.0,   # normalizado
            user_state.session_length / 60.0,  # normalizado para 1 hora
            user_state.avg_session_length / 60.0,
            user_state.return_rate,
            user_state.conversion_rate
        ])
        
        # Adicionar √∫ltimas intera√ß√µes (encoded como √≠ndices de itens)
        recent_items = user_state.last_interactions
        for item_id in recent_items:
            if item_id in self.item_library.items:
                state_vector.extend(self.item_library.items[item_id]['embedding'])
        
        # Padding se necess√°rio para manter tamanho fixo
        # ...
        
        return np.array(state_vector)
    
    def step(self, action):
        """
        Executa uma a√ß√£o (recomenda√ß√£o) e retorna:
        - pr√≥ximo estado
        - recompensa
        - flag de t√©rmino
        - informa√ß√µes adicionais
        """
        self.step_count += 1
        
        # Mapear √≠ndice de a√ß√£o para ID de item
        item_candidates = self.item_library.get_candidate_items(
            self.current_state, n=self.action_space.n
        )
        
        if action < len(item_candidates):
            item_id = item_candidates[action]
        else:
            # Fallback se a√ß√£o estiver fora do range
            item_id = random.choice(item_candidates) if item_candidates else None
        
        # Verificar se item j√° foi recomendado neste epis√≥dio
        if item_id in self.recommended_items:
            reward = -0.5  # penalidade por recomendar o mesmo item
        else:
            # Simular feedback do usu√°rio
            reward = self.simulate_user_feedback(item_id)
            self.recommended_items.append(item_id)
        
        # Atualizar estado
        history = self.user_data[self.current_user]['history']
        context = self.user_data[self.current_user]['context']
        
        # Atualizar contexto com novas informa√ß√µes
        if item_id:
            context['last_interactions'].append(item_id)
            context['session_length'] += 1
        
        # Criar novo estado
        next_state = self.create_state_representation(
            self.current_user, history, context
        )
        
        # Verificar se epis√≥dio terminou
        done = self.step_count >= self.max_steps
        
        # Informa√ß√µes adicionais
        info = {
            'item_id': item_id,
            'user_id': self.current_user,
            'reward_details': {
                'click': 1 if reward > 0 else 0,
                'engagement': max(0, reward - 0.5)
            }
        }
        
        self.current_state = next_state
        
        return next_state, reward, done, info
    
    def simulate_user_feedback(self, item_id):
        """
        Simula a resposta do usu√°rio √† recomenda√ß√£o
        Retorna uma recompensa baseada na probabilidade de interesse
        """
        if not item_id:
            return -1.0  # penalidade por recomenda√ß√£o inv√°lida
        
        user_state = UserState(
            self.current_user,
            self.user_data[self.current_user]['history'],
            self.user_data[self.current_user]['context']
        )
        
        # Calcular probabilidade de interesse baseada em:
        
        # 1. Similaridade com hist√≥rico recente
        similarity_score = 0
        if item_id in self.item_library.items:
            item_embedding = self.item_library.items[item_id]['embedding']
            similarity_score = cosine_similarity(item_embedding, user_state.item_embedding_avg)
        
        # 2. Prefer√™ncia por categoria
        category_score = 0
        if item_id in self.item_library.items:
            item_categories = self.item_library.items[item_id]['categories']
            category_scores = [
                user_state.category_preferences.get(cat, 0) 
                for cat in item_categories
            ]
            category_score = max(category_scores) if category_scores else 0
        
        # 3. Fator de novidade/surpresa
        novelty_score = 1.0
        for hist_item in user_state.short_term_history:
            if hist_item in self.item_library.items:
                hist_embedding = self.item_library.items[hist_item]['embedding']
                if item_id in self.item_library.items:
                    item_embedding = self.item_library.items[item_id]['embedding']
                    similarity = cosine_similarity(hist_embedding, item_embedding)
                    novelty_score = min(novelty_score, 1.0 - similarity)
        
        # Combinar scores
        interest_probability = (
            0.5 * similarity_score + 
            0.3 * category_score + 
            0.2 * novelty_score
        )
        
        # Adicionar ru√≠do para simular comportamento n√£o deterministico
        interest_probability = min(1.0, max(0.0, 
            interest_probability + np.random.normal(0, 0.1)
        ))
        
        # Simular clique (recompensa bin√°ria)
        click = np.random.random() < interest_probability
        
        # Simular engajamento (recompensa cont√≠nua)
        if click:
            engagement = interest_probability * 2  # Escala: 0 - 2
            return 1.0 + engagement  # Recompensa total: 1 (clique) + engajamento
        else:
            return 0.0  # Sem clique, sem recompensa
```

### Treinamento e Implementa√ß√£o do Agente

```python
# Configura√ß√£o do ambiente e agente
def train_recommendation_agent(user_data, item_library, episodes=1000):
    """Treina o agente de recomenda√ß√£o usando DQN"""
    # Criar ambiente
    env = RecommendationEnvironment(user_data, item_library)
    
    # Obter dimens√µes do problema
    state_dim = env.reset()[0].shape[0]
    action_dim = 100  # Considerar top-100 candidatos como a√ß√µes poss√≠veis
    
    # Criar agente
    agent = DuelingDQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=0.0005,
        gamma=0.95,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01,
        batch_size=32,
        memory_size=50000
    )
    
    # M√©tricas de treinamento
    rewards_history = []
    avg_rewards_history = []
    
    # Loop de treinamento
    for episode in range(episodes):
        state, _ = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            # Selecionar a√ß√£o
            action = agent.act(state)
            
            # Executar a√ß√£o
            next_state, reward, done, info = env.step(action)
            
            # Armazenar experi√™ncia
            agent.remember(state, action, reward, next_state, done)
            
            # Treinar agente
            agent.replay()
            
            state = next_state
            total_reward += reward
        
        # Atualizar modelo alvo periodicamente
        if episode % 10 == 0:
            agent.update_target_model()
        
        # Registrar recompensas
        rewards_history.append(total_reward)
        avg_reward = np.mean(rewards_history[-100:])
        avg_rewards_history.append(avg_reward)
        
        # Exibir progresso
        if episode % 20 == 0:
            print(f"Epis√≥dio: {episode}, Recompensa: {total_reward:.2f}, "
                  f"M√©dia (100 ep): {avg_reward:.2f}, Epsilon: {agent.epsilon:.4f}")
        
        # Salvar modelo periodicamente
        if episode % 100 == 0:
            agent.save(f"recommendation_model_ep{episode}.h5")
    
    # Salvar modelo final
    agent.save("recommendation_model_final.h5")
    
    return agent, rewards_history, avg_rewards_history
```

## üìè M√©tricas de Avalia√ß√£o

### M√©tricas Online

- **CTR (Click-Through Rate)**: Taxa de cliques nas recomenda√ß√µes
- **Tempo de Sess√£o**: Dura√ß√£o m√©dia das sess√µes de usu√°rio
- **Profundidade de Scroll**: Quantas recomenda√ß√µes s√£o visualizadas
- **Conversion Rate**: Taxa de convers√£o ap√≥s recomenda√ß√µes 
- **Retention Rate**: Taxa de retorno dos usu√°rios

### M√©tricas Offline

- **Precision@K**: Precis√£o das recomenda√ß√µes nos top-K itens
- **Recall@K**: Cobertura das recomenda√ß√µes nos top-K itens
- **NDCG**: Discounted Cumulative Gain normalizado
- **Diversity**: Diversidade das recomenda√ß√µes
- **Serendipity**: Capacidade de recomendar itens surpreendentes mas relevantes

## üåü Aspectos Avan√ßados

### Contextual Multi-Armed Bandits

Para cen√°rios com menos dados dispon√≠veis ou necessidade de adapta√ß√£o mais r√°pida:

```mermaid
graph LR
    A[Estado/Contexto] --> B[Modelo de Bandits]
    B --> C[Sele√ß√£o de Item]
    C --> D[Feedback]
    D --> E[Atualiza√ß√£o de Modelo]
    E --> B
```

### Recomenda√ß√£o Multi-objetivo

```mermaid
graph TD
    A[M√∫ltiplos Objetivos] --> A1[Engajamento de Curto Prazo]
    A --> A2[Reten√ß√£o de Longo Prazo]
    A --> A3[Diversidade de Conte√∫do]
    A --> A4[Monetiza√ß√£o]
    
    A1 & A2 & A3 & A4 --> B[Escalariza√ß√£o Din√¢mica]
    B --> C[Recompensa Composta]
    C --> D[Agente RL]
```

### Exploration Strategies

```mermaid
graph TD
    A[Estrat√©gias de Explora√ß√£o] --> B[Thompson Sampling]
    A --> C[UCB (Upper Confidence Bound)]
    A --> D[Noisy Networks]
    A --> E[Entropy-Based Exploration]
    
    B & C & D & E --> F[Pol√≠tica de Recomenda√ß√£o]
```

## üåê Aplica√ß√µes em Diferentes Setores

### E-commerce

```mermaid
graph TD
    A[E-commerce] --> B[Recomenda√ß√£o de Produtos]
    A --> C[Upselling/Cross-selling]
    A --> D[Recomenda√ß√£o de Promo√ß√µes]
```

### Streaming de Conte√∫do

```mermaid
graph TD
    A[Plataformas de Streaming] --> B[Recomenda√ß√£o de Conte√∫do]
    A --> C[Sequ√™ncia de Visualiza√ß√£o]
    A --> D[Descoberta de Conte√∫do]
```

### Redes Sociais

```mermaid
graph TD
    A[Redes Sociais] --> B[Feed de Conte√∫do]
    A --> C[Conex√µes/Amizades]
    A --> D[Conte√∫do Trending]
```

## üîç Considera√ß√µes e Desafios

### Desafios T√©cnicos

- **Cold Start**: Recomenda√ß√µes para novos usu√°rios/itens
- **Feedback Escasso**: Nem todas as intera√ß√µes geram feedback expl√≠cito
- **Escalabilidade**: Milh√µes de usu√°rios e itens
- **Lat√™ncia**: Recomenda√ß√µes devem ser geradas em milissegundos

### Aspectos √âticos

- **Filter Bubbles**: Evitar refor√ßo excessivo de prefer√™ncias existentes
- **Manipula√ß√£o**: Balancear interesses do usu√°rio e objetivos do neg√≥cio
- **Transpar√™ncia**: Explicabilidade das recomenda√ß√µes
- **Privacidade**: Tratamento respons√°vel dos dados de usu√°rio

## üìà Resultados Esperados

- Aumento de 15-30% no engajamento do usu√°rio
- Melhoria de 20-40% no tempo de sess√£o m√©dio
- Incremento de 10-25% nas taxas de convers√£o
- Crescimento de 15-30% na diversidade de conte√∫do consumido
- Aumento de 20-35% na reten√ß√£o de usu√°rios

## üöÄ Evolu√ß√£o e Pr√≥ximos Passos

- **Personaliza√ß√£o em Multin√≠veis**: Recomenda√ß√µes em diferentes horizontes temporais
- **Recomenda√ß√£o Multimodal**: Integrar texto, imagem, √°udio e v√≠deo
- **Aprendizado Federado**: Treinamento sem centraliza√ß√£o de dados
- **Adapta√ß√£o Cont√≠nua**: Modelos que evoluem em tempo real
- **Explicabilidade**: Justificativas compreens√≠veis para recomenda√ß√µes