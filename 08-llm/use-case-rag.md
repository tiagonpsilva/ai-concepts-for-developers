# üîç Caso de Uso: Retrieval-Augmented Generation (RAG)

## üéØ Objetivo

Desenvolver um sistema de Retrieval-Augmented Generation (RAG) que melhore a precis√£o, atualidade e confiabilidade das respostas de LLMs, combinando os modelos de linguagem com a capacidade de buscar e incorporar informa√ß√µes de fontes externas.

## üîç Problema de Neg√≥cio

Os LLMs enfrentam limita√ß√µes significativas que impactam sua utilidade em cen√°rios empresariais:

- Conhecimento limitado ao per√≠odo de treinamento, sem acesso a informa√ß√µes recentes
- Tend√™ncia a "alucinar" ou fabricar informa√ß√µes quando confrontados com perguntas fora de seu dom√≠nio
- Dificuldade em citar fontes precisas para suas respostas
- Incapacidade de acessar informa√ß√µes propriet√°rias ou espec√≠ficas de uma organiza√ß√£o
- Limita√ß√µes na rec√™ncia e relev√¢ncia de informa√ß√µes para dom√≠nios em r√°pida evolu√ß√£o

O RAG resolve esses problemas ao permitir que os LLMs acessem bases de conhecimento externas em tempo real, melhorando drasticamente a precis√£o, atualidade e confiabilidade das respostas, especialmente para casos de uso empresariais que exigem informa√ß√µes espec√≠ficas e verific√°veis.

## üß© Como Funciona o RAG

```mermaid
graph TD
    A[Pergunta do Usu√°rio] --> B[Processamento de Consulta]
    B --> C[Busca em Base de Conhecimento]
    C --> D[Recupera√ß√£o de Documentos Relevantes]
    D --> E[Reranking e Filtragem]
    E --> F[Constru√ß√£o de Prompt Aumentado]
    F --> G[Infer√™ncia do LLM]
    G --> H[P√≥s-processamento]
    H --> I[Resposta Final]
```

### Componentes Principais

```mermaid
graph TD
    A[Componentes RAG] --> B[Base de Conhecimento]
    A --> C[Sistema de Recupera√ß√£o]
    A --> D[LLM]
    A --> E[Orquestrador]
    
    B --> B1[Indexa√ß√£o de Documentos]
    B --> B2[Embeddings Vetoriais]
    B --> B3[Metadados e Filtragem]
    
    C --> C1[Busca Sem√¢ntica]
    C --> C2[Busca por Palavras-chave]
    C --> C3[Reranking]
    
    D --> D1[Gera√ß√£o de Resposta]
    D --> D2[Chain-of-Thought]
    
    E --> E1[Constru√ß√£o de Contexto]
    E --> E2[Gerenciamento de Fontes]
```

## üõ†Ô∏è Arquitetura de um Sistema RAG

### Diagrama de Arquitetura

```mermaid
graph TD
    subgraph "Fase de Indexa√ß√£o"
        A1[Documentos Brutos] --> A2[Extra√ß√£o de Texto]
        A2 --> A3[Chunking/Segmenta√ß√£o]
        A3 --> A4[Gera√ß√£o de Embeddings]
        A4 --> A5[Armazenamento em Vector Store]
    end
    
    subgraph "Fase de Consulta"
        B1[Consulta do Usu√°rio] --> B2[Processamento de Consulta]
        B2 --> B3[Embedding de Consulta]
        B3 --> B4[Busca Sem√¢ntica]
        A5 -.-> B4
        B4 --> B5[Reranking de Resultados]
        B5 --> B6[Constru√ß√£o de Prompt]
        B6 --> B7[Chamada ao LLM]
        B7 --> B8[Formata√ß√£o de Resposta]
    end
```

### Pipeline de Processamento de Documentos

```mermaid
graph LR
    A[Documentos] --> B[Carregadores]
    B --> C[Transformadores]
    C --> D[Segmentadores]
    D --> E[Embeddings]
    E --> F[Armazenamento Vetorial]
    
    B --> B1[PDF]
    B --> B2[Web]
    B --> B3[DB]
    
    C --> C1[HTML ‚Üí Text]
    C --> C2[Tabelas ‚Üí Text]
    
    D --> D1[Por Tamanho]
    D --> D2[Por Sem√¢ntica]
    
    E --> E1[OpenAI]
    E --> E2[SentenceTransformers]
    
    F --> F1[Pinecone]
    F --> F2[Chroma]
    F --> F3[FAISS]
```

## üíª Implementa√ß√£o

Aqui est√° uma implementa√ß√£o b√°sica de um sistema RAG usando Python e bibliotecas populares:

```python
import os
import time
from typing import List, Dict, Any, Optional
import logging

# Processamento de documentos
from langchain.document_loaders import PyPDFLoader, WebBaseLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Embeddings e armazenamento vetorial
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# LLM
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

# Utils
import numpy as np

class RAGSystem:
    """Sistema de Retrieval-Augmented Generation (RAG)"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        embedding_model: str = "text-embedding-ada-002",
        llm_model: str = "gpt-4",
        vector_store_path: str = "./vector_store",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """
        Inicializa o sistema RAG
        
        Args:
            api_key: Chave de API para o provedor do LLM
            embedding_model: Modelo para gera√ß√£o de embeddings
            llm_model: Modelo de linguagem para gera√ß√£o de respostas
            vector_store_path: Caminho para o armazenamento vetorial
            chunk_size: Tamanho dos chunks de texto
            chunk_overlap: Sobreposi√ß√£o entre chunks adjacentes
        """
        # Configura√ß√£o de API
        if api_key:
            os.environ["OPENAI_API_KEY"] = api_key
        elif os.environ.get("OPENAI_API_KEY") is None:
            raise ValueError("API key required")
        
        # Configura√ß√£o do logger
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger("RAGSystem")
        
        # Embeddings e Vector Store
        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.vector_store_path = vector_store_path
        self.vector_store = None
        
        # Configura√ß√£o de Chunking
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len
        )
        
        # LLM para gera√ß√£o
        self.llm = ChatOpenAI(model=llm_model, temperature=0.2)
        
        # Estado do sistema
        self.is_vector_store_loaded = False
        self.document_sources = {}
        
        # Template de prompts
        self.qa_template = """
        Voc√™ √© um assistente AI preciso que responde perguntas baseadas em fontes confi√°veis.
        Use as informa√ß√µes fornecidas no CONTEXTO abaixo para responder √† PERGUNTA.
        Se o CONTEXTO n√£o contiver informa√ß√µes suficientes, diga que n√£o tem informa√ß√µes suficientes.
        N√£o invente ou fabrique informa√ß√µes que n√£o est√£o no CONTEXTO.
        Sempre cite as fontes usadas na sua resposta.
        
        CONTEXTO:
        {context}
        
        PERGUNTA:
        {question}
        
        RESPOSTA:
        """
    
    def load_document(self, document_path: str, document_type: str = "auto", metadata: Optional[Dict] = None) -> List[Document]:
        """
        Carrega um documento para processamento
        
        Args:
            document_path: Caminho ou URL do documento
            document_type: Tipo de documento ('pdf', 'web', 'text' ou 'auto')
            metadata: Metadados adicionais para o documento
            
        Returns:
            Lista de documentos carregados
        """
        # Determinar automaticamente o tipo se necess√°rio
        if document_type == "auto":
            if document_path.startswith("http"):
                document_type = "web"
            elif document_path.endswith(".pdf"):
                document_type = "pdf"
            elif document_path.endswith(".txt"):
                document_type = "text"
            else:
                document_type = "text"  # default
        
        # Carregar documento com o loader apropriado
        try:
            if document_type == "pdf":
                loader = PyPDFLoader(document_path)
            elif document_type == "web":
                loader = WebBaseLoader(document_path)
            elif document_type == "text":
                loader = TextLoader(document_path)
            else:
                raise ValueError(f"Unsupported document type: {document_type}")
            
            documents = loader.load()
            self.logger.info(f"Loaded document: {document_path} ({len(documents)} pages/sections)")
            
            # Adicionar metadados
            if metadata:
                for doc in documents:
                    doc.metadata.update(metadata)
            
            # Adicionar fonte ao registro
            doc_id = str(len(self.document_sources) + 1)
            self.document_sources[doc_id] = {
                "path": document_path,
                "type": document_type,
                "num_sections": len(documents)
            }
            
            # Adicionar ID de documento aos metadados
            for doc in documents:
                doc.metadata['doc_id'] = doc_id
            
            return documents
            
        except Exception as e:
            self.logger.error(f"Error loading document {document_path}: {e}")
            return []
    
    def process_documents(self, documents: List[Document]) -> List[Document]:
        """
        Processa documentos dividindo-os em chunks menores
        
        Args:
            documents: Lista de documentos a processar
            
        Returns:
            Lista de chunks de documento
        """
        try:
            chunks = self.text_splitter.split_documents(documents)
            self.logger.info(f"Split {len(documents)} documents into {len(chunks)} chunks")
            return chunks
        except Exception as e:
            self.logger.error(f"Error processing documents: {e}")
            return []
    
    def add_documents_to_index(self, documents: List[Document], create_new: bool = False) -> bool:
        """
        Adiciona documentos ao √≠ndice vetorial
        
        Args:
            documents: Lista de documentos a adicionar
            create_new: Se deve criar um novo √≠ndice (apagando o existente)
            
        Returns:
            True se adicionado com sucesso, False caso contr√°rio
        """
        try:
            # Processar documentos
            chunks = self.process_documents(documents)
            if not chunks:
                return False
            
            # Criar ou atualizar vector store
            if create_new or not self.is_vector_store_loaded:
                # Criar novo vector store
                self.vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=self.vector_store_path
                )
                self.is_vector_store_loaded = True
                self.logger.info(f"Created new vector store with {len(chunks)} chunks")
            else:
                # Adicionar a vector store existente
                self.vector_store.add_documents(chunks)
                self.logger.info(f"Added {len(chunks)} chunks to existing vector store")
            
            # Persistir altera√ß√µes
            if hasattr(self.vector_store, '_persist'):
                self.vector_store._persist()
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error adding documents to index: {e}")
            return False
    
    def load_vector_store(self) -> bool:
        """
        Carrega um vector store existente
        
        Returns:
            True se carregado com sucesso, False caso contr√°rio
        """
        try:
            self.vector_store = Chroma(
                persist_directory=self.vector_store_path,
                embedding_function=self.embeddings
            )
            self.is_vector_store_loaded = True
            collection_count = self.vector_store._collection.count()
            self.logger.info(f"Loaded vector store with {collection_count} entries")
            return True
        except Exception as e:
            self.logger.error(f"Error loading vector store: {e}")
            return False
    
    def retrieve_relevant_chunks(
        self, 
        query: str, 
        top_k: int = 5,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> List[Document]:
        """
        Recupera chunks relevantes para uma consulta
        
        Args:
            query: Consulta do usu√°rio
            top_k: N√∫mero de resultados a retornar
            filter_criteria: Crit√©rios de filtragem (metadados)
            
        Returns:
            Lista de documentos relevantes
        """
        if not self.is_vector_store_loaded:
            if not self.load_vector_store():
                self.logger.error("No vector store available for retrieval")
                return []
        
        try:
            results = self.vector_store.similarity_search(
                query=query,
                k=top_k,
                filter=filter_criteria
            )
            
            self.logger.info(f"Retrieved {len(results)} chunks for query: {query}")
            return results
        except Exception as e:
            self.logger.error(f"Error retrieving chunks: {e}")
            return []
    
    def generate_answer(
        self, 
        query: str, 
        retrieved_docs: List[Document],
        max_context_length: int = 3800
    ) -> Dict[str, Any]:
        """
        Gera uma resposta baseada nos documentos recuperados
        
        Args:
            query: Consulta do usu√°rio
            retrieved_docs: Documentos recuperados
            max_context_length: Comprimento m√°ximo de contexto
            
        Returns:
            Dicion√°rio com resposta e metadados
        """
        if not retrieved_docs:
            return {
                "answer": "N√£o tenho informa√ß√µes suficientes para responder a essa pergunta.",
                "sources": [],
                "has_answer": False
            }
        
        # Preparar contexto
        context_str = ""
        used_docs = []
        
        for doc in retrieved_docs:
            doc_str = f"Fonte {doc.metadata.get('doc_id', 'desconhecida')}"
            
            if 'source' in doc.metadata:
                doc_str += f" ({doc.metadata['source']})"
            
            doc_str += f": {doc.page_content}\n\n"
            
            # Verificar se adicionar este doc excederia o limite
            if len(context_str) + len(doc_str) > max_context_length:
                break
            
            context_str += doc_str
            used_docs.append(doc)
        
        # Verificar se temos algum contexto
        if not context_str:
            return {
                "answer": "O contexto √© muito grande para processar. Por favor, refine sua pergunta.",
                "sources": [],
                "has_answer": False
            }
        
        # Criar prompt completo
        prompt = ChatPromptTemplate.from_template(self.qa_template)
        messages = prompt.format_messages(context=context_str, question=query)
        
        # Gerar resposta
        try:
            start_time = time.time()
            response = self.llm(messages)
            gen_time = time.time() - start_time
            
            # Extrair fontes usadas
            sources = []
            for doc in used_docs:
                source_info = {
                    "doc_id": doc.metadata.get('doc_id', 'unknown'),
                    "source": doc.metadata.get('source', self.document_sources.get(doc.metadata.get('doc_id', ''), {}).get('path', 'unknown'))
                }
                
                # Adicionar metadados adicionais relevantes
                for key, value in doc.metadata.items():
                    if key not in ['doc_id', 'source'] and not key.startswith('_'):
                        source_info[key] = value
                
                sources.append(source_info)
            
            return {
                "answer": response.content,
                "sources": sources,
                "generation_time": gen_time,
                "has_answer": True,
                "num_docs_used": len(used_docs),
                "num_docs_retrieved": len(retrieved_docs)
            }
            
        except Exception as e:
            self.logger.error(f"Error generating answer: {e}")
            return {
                "answer": "Ocorreu um erro ao gerar a resposta.",
                "sources": [],
                "error": str(e),
                "has_answer": False
            }
    
    def answer_query(
        self, 
        query: str, 
        top_k: int = 5,
        filter_criteria: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Processa uma consulta e gera uma resposta usando RAG
        
        Args:
            query: Consulta do usu√°rio
            top_k: N√∫mero de documentos a recuperar
            filter_criteria: Filtros a aplicar na recupera√ß√£o
            
        Returns:
            Resposta e metadados
        """
        self.logger.info(f"Processing query: {query}")
        
        # Recuperar documentos relevantes
        retrieved_docs = self.retrieve_relevant_chunks(query, top_k, filter_criteria)
        
        # Gerar resposta
        result = self.generate_answer(query, retrieved_docs)
        
        return result
    
    def index_directory(
        self, 
        directory_path: str, 
        file_types: List[str] = ['.pdf', '.txt'],
        recursive: bool = True,
        metadata: Optional[Dict] = None
    ) -> int:
        """
        Indexa todos os documentos em um diret√≥rio
        
        Args:
            directory_path: Caminho do diret√≥rio
            file_types: Extens√µes de arquivo a indexar
            recursive: Se deve buscar em subdiret√≥rios
            metadata: Metadados a aplicar aos documentos
            
        Returns:
            N√∫mero de documentos indexados
        """
        indexed_count = 0
        
        try:
            import os
            
            # Caminhos a percorrer
            paths_to_walk = []
            if recursive:
                for root, _, files in os.walk(directory_path):
                    paths_to_walk.append((root, files))
            else:
                paths_to_walk.append((directory_path, os.listdir(directory_path)))
            
            # Indexar cada arquivo relevante
            all_documents = []
            
            for root, files in paths_to_walk:
                for file in files:
                    if any(file.endswith(ext) for ext in file_types):
                        file_path = os.path.join(root, file)
                        
                        # Definir tipo de documento
                        doc_type = "auto"
                        if file.endswith('.pdf'):
                            doc_type = "pdf"
                        elif file.endswith('.txt'):
                            doc_type = "text"
                        
                        # Carregar documento
                        docs = self.load_document(file_path, doc_type, metadata)
                        if docs:
                            all_documents.extend(docs)
                            indexed_count += 1
            
            # Adicionar todos os documentos ao √≠ndice
            if all_documents:
                self.add_documents_to_index(all_documents)
            
            self.logger.info(f"Indexed {indexed_count} documents from {directory_path}")
            return indexed_count
            
        except Exception as e:
            self.logger.error(f"Error indexing directory {directory_path}: {e}")
            return indexed_count

# Exemplo de uso
if __name__ == "__main__":
    # Inicializar sistema RAG
    rag_system = RAGSystem()
    
    # Indexar documentos
    rag_system.load_document("example_docs/company_policy.pdf", "pdf", {"category": "policy"})
    rag_system.load_document("example_docs/product_manual.pdf", "pdf", {"category": "manual"})
    
    # Adicionar documentos ao √≠ndice
    rag_system.add_documents_to_index(rag_system.document_sources.values())
    
    # Testar uma consulta
    result = rag_system.answer_query("Qual √© a pol√≠tica de devolu√ß√£o da empresa?")
    
    print(f"Resposta: {result['answer']}")
    print("\nFontes utilizadas:")
    for source in result['sources']:
        print(f"- {source['source']}")
```

## üìä Otimiza√ß√µes Avan√ßadas

### Chunking Estrat√©gico

```mermaid
graph LR
    A[Estrat√©gias de Chunking] --> B[Por Tamanho Fixo]
    A --> C[Por Sem√¢ntica]
    A --> D[Por Estrutura]
    A --> E[Hier√°rquico]
    
    B --> B1[Tokens]
    B --> B2[Caracteres]
    
    C --> C1[Par√°grafos]
    C --> C2[T√≥picos]
    
    D --> D1[HTML/XML]
    D --> D2[Markdown]
    
    E --> E1[Parent-Child]
    E --> E2[Recursivo]
```

### Reranking e Fus√£o

```mermaid
graph TD
    A[Recupera√ß√£o H√≠brida] --> B[Busca Vetorial]
    A --> C[Busca por Palavras-chave]
    B & C --> D[Fus√£o de Resultados]
    D --> E[Reranking]
    E --> F[Top-K Final]
    
    E --> E1[Cross-encoder]
    E --> E2[Reciprocal Rank Fusion]
```

### Feedback Humano e Avalia√ß√£o

```mermaid
graph TD
    A[Ciclo de Feedback] --> B[Uso do Sistema]
    B --> C[Avalia√ß√£o de Resposta]
    C --> D[Coleta de Feedback]
    D --> E[An√°lise de Qualidade]
    E --> F[Ajustes no Sistema]
    F --> B
```

## üåê Casos de Uso Espec√≠ficos

### Assistente de Conhecimento Corporativo

```mermaid
graph TD
    A[Fontes Corporativas] --> A1[Pol√≠ticas Internas]
    A --> A2[Documenta√ß√£o T√©cnica]
    A --> A3[Base de Conhecimento]
    A --> A4[Emails e Comunica√ß√µes]
    
    A1 & A2 & A3 & A4 --> B[Sistema RAG]
    
    B --> C[Atendimento ao Colaborador]
    B --> D[Suporte T√©cnico Interno]
    B --> E[Onboarding]
    B --> F[Consulta de Normas]
```

### Suporte ao Cliente Avan√ßado

```mermaid
graph TD
    A[Fontes de Suporte] --> A1[Manuais de Produto]
    A --> A2[FAQs]
    A --> A3[Tickets Anteriores]
    A --> A4[F√≥runs de Discuss√£o]
    
    A1 & A2 & A3 & A4 --> B[Sistema RAG]
    
    B --> C[Chatbot de Suporte]
    B --> D[Agentes de Suporte]
    B --> E[Self-service]
    B --> F[An√°lise de Problemas]
```

### Pesquisa e An√°lise Legal

```mermaid
graph TD
    A[Documentos Legais] --> A1[Legisla√ß√£o]
    A --> A2[Jurisprud√™ncia]
    A --> A3[Contratos]
    A --> A4[Pareceres]
    
    A1 & A2 & A3 & A4 --> B[Sistema RAG]
    
    B --> C[An√°lise de Contratos]
    B --> D[Due Diligence]
    B --> E[Pesquisa Jur√≠dica]
    B --> F[Avalia√ß√£o de Riscos]
```

## üìè M√©tricas de Avalia√ß√£o

- **Precis√£o**: Acur√°cia factual das respostas geradas
- **Relev√¢ncia**: Alinhamento da resposta com a consulta do usu√°rio
- **Completude**: Cobertura de todos os aspectos relevantes da consulta
- **Cita√ß√£o**: Precis√£o e validade das fontes citadas
- **Tempo de Resposta**: Lat√™ncia total do sistema
- **Taxa de Hallucination**: Frequ√™ncia de informa√ß√µes fabricadas

## üõ°Ô∏è Considera√ß√µes Importantes

### Limita√ß√µes e Desafios

- **Depend√™ncia da Qualidade dos Dados**: O sistema s√≥ √© t√£o bom quanto as fontes indexadas
- **Interpreta√ß√£o de Consultas**: Entender corretamente a inten√ß√£o do usu√°rio
- **Lat√™ncia**: O processo de recupera√ß√£o adiciona overhead ao tempo de resposta
- **Manuten√ß√£o da Base de Conhecimento**: Necessidade de atualiza√ß√£o cont√≠nua
- **Escalabilidade de Armazenamento**: Desafios com bases de conhecimento muito grandes

### Privacidade e Seguran√ßa

```mermaid
graph TD
    A[Considera√ß√µes] --> B[Dados Sens√≠veis]
    A --> C[Controle de Acesso]
    A --> D[Confidencialidade]
    A --> E[Auditoria]
    
    B --> B1[PII/PHI]
    B --> B2[IP Propriet√°rio]
    
    C --> C1[RBAC]
    C --> C2[Filtragem Contextual]
    
    D --> D1[Encripta√ß√£o]
    D --> D2[Acesso Baseado em Necessidade]
    
    E --> E1[Logs de Uso]
    E --> E2[Rastreabilidade]
```

## üîÑ Tend√™ncias Futuras

- **RAG Multi-hop**: Consultas encadeadas para racioc√≠nio mais complexo
- **Alucina√ß√£o Controlada**: Melhores t√©cnicas para mitigar fabrica√ß√£o de informa√ß√µes
- **LLM Fine-tuning para RAG**: Modelos especializados em integrar conte√∫do externo
- **RAG Multimodal**: Incorpora√ß√£o de informa√ß√µes de imagens, √°udio e v√≠deo
- **Gera√ß√£o e Atualiza√ß√£o Autom√°tica de Conhecimento**: Bases de conhecimento auto-atualiz√°veis