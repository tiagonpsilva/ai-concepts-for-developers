# üîÑ Transformers

Transformers s√£o uma arquitetura de rede neural que revolucionou o processamento de linguagem natural e, mais recentemente, outras modalidades como vis√£o computacional e dados de s√©ries temporais. Sua inova√ß√£o principal √© o mecanismo de self-attention, que permite processar sequ√™ncias de dados com efici√™ncia sem comprometer a capacidade de capturar depend√™ncias de longo alcance.

## üìë Defini√ß√£o

A arquitetura Transformer, introduzida no artigo "Attention is All You Need" (2017), √© um modelo baseado somente em mecanismos de aten√ß√£o, eliminando as estruturas recorrentes e convolucionais previamente dominantes. Essa abordagem permite processamento paralelo de sequ√™ncias inteiras, resultando em treinamento mais r√°pido e desempenho superior em tarefas de sequ√™ncia.

## üß† Arquitetura do Transformer

```mermaid
graph TD
    subgraph "Transformer"
        A[Input Embedding + Positional Encoding] --> B[Encoder]
        A --> C[Decoder]
        B --> C
        C --> D[Output Probabilities]
    end
    
    subgraph "Encoder Block"
        E[Multi-Head Self-Attention] --> F[Add & Norm]
        F --> G[Feed Forward Network]
        G --> H[Add & Norm]
    end
    
    subgraph "Decoder Block"
        I[Masked Multi-Head Self-Attention] --> J[Add & Norm]
        J --> K[Multi-Head Attention over Encoder]
        K --> L[Add & Norm]
        L --> M[Feed Forward Network]
        M --> N[Add & Norm]
    end
```

## üß© Componentes Fundamentais

### Self-Attention

```mermaid
graph LR
    A[Input] --> B[Query]
    A --> C[Key]
    A --> D[Value]
    B & C --> E[Attention Weights]
    E & D --> F[Weighted Sum]
    F --> G[Output]
```

O mecanismo de self-attention permite que cada elemento em uma sequ√™ncia "preste aten√ß√£o" a todos os outros elementos, capturando rela√ß√µes complexas:

1. Para cada posi√ß√£o, o modelo calcula tr√™s vetores:
   - **Query (Q)**: O que a posi√ß√£o atual est√° "procurando"
   - **Key (K)**: O que cada posi√ß√£o "oferece"
   - **Value (V)**: A informa√ß√£o de cada posi√ß√£o

2. As pontua√ß√µes de aten√ß√£o s√£o calculadas como o produto escalar entre Query e todas as Keys, resultando em um "mapa de aten√ß√£o"

3. Essas pontua√ß√µes s√£o normalizadas e usadas para criar uma m√©dia ponderada dos Values

### Multi-Head Attention

```mermaid
graph TD
    A[Input] --> B1[Attention Head 1]
    A --> B2[Attention Head 2]
    A --> B3[Attention Head 3]
    A --> B4[Attention Head n]
    
    B1 & B2 & B3 & B4 --> C[Concatenate]
    C --> D[Linear Projection]
    D --> E[Output]
```

Multi-Head Attention permite que o modelo capture diferentes tipos de rela√ß√µes simultaneamente, usando m√∫ltiplos conjuntos de matrizes de proje√ß√£o Q, K e V.

### Positional Encoding

```mermaid
graph LR
    A[Token Embedding] --> C[+]
    B[Positional Encoding] --> C
    C --> D[Embedded Tokens with Position]
```

Como o Transformer processa tokens em paralelo (sem ordem sequencial), informa√ß√µes posicionais s√£o adicionadas explicitamente √†s embeddings atrav√©s de fun√ß√µes seno e cosseno de diferentes frequ√™ncias.

### Layer Normalization & Residual Connections

```mermaid
graph LR
    A[Input] --> B[Sublayer]
    A --> C[+]
    B --> C
    C --> D[Layer Normalization]
    D --> E[Output]
```

Estas t√©cnicas ajudam a estabilizar o treinamento de redes profundas:
- **Residual Connections**: Adicionam a entrada original √† sa√≠da de cada sublayer
- **Layer Normalization**: Normaliza as ativa√ß√µes em cada camada

## üîÑ Variantes de Transformers

```mermaid
graph TD
    A[Transformers] --> B[Encoder-Only]
    A --> C[Decoder-Only]
    A --> D[Encoder-Decoder]
    
    B --> B1[BERT]
    B --> B2[RoBERTa]
    B --> B3[DeBERTa]
    
    C --> C1[GPT]
    C --> C2[LLaMA]
    C --> C3[Falcon]
    
    D --> D1[T5]
    D --> D2[BART]
    D --> D3[PaLM]
```

### Encoder-Only
Focados em entender o contexto completo de uma sequ√™ncia, √≥timos para classifica√ß√£o, extra√ß√£o de informa√ß√µes e outras tarefas de compreens√£o.

### Decoder-Only
Especializados em gera√ß√£o de texto, utilizados em LLMs como GPT, Claude e outros modelos generativos.

### Encoder-Decoder
Combinam as vantagens de ambos, ideal para tarefas que requerem tanto compreens√£o quanto gera√ß√£o, como tradu√ß√£o e resumo.

## üõ†Ô∏è Aplica√ß√µes dos Transformers

- **Processamento de Linguagem Natural**:
  - Tradu√ß√£o autom√°tica
  - Gera√ß√£o de texto
  - Classifica√ß√£o de texto
  - Resposta a perguntas
  - Sumariza√ß√£o
  
- **Vis√£o Computacional**:
  - Classifica√ß√£o de imagens
  - Detec√ß√£o de objetos
  - Segmenta√ß√£o
  - Vis√£o-linguagem (CLIP, GPT-4V)
  
- **√Åudio e Fala**:
  - Reconhecimento de fala
  - Gera√ß√£o de fala
  - M√∫sica e √°udio

- **Multimodal**:
  - Processamento conjunto de texto, imagem, √°udio
  - Entendimento multimodal

## üîç T√©cnicas Avan√ßadas

### Attention Masks

```mermaid
graph TD
    A[Attention Scores] --> B[Mask Application]
    B --> C[Masked Scores]
    C --> D[Softmax]
    D --> E[Attention Weights]
```

Permitem controlar quais tokens podem atender a quais outros, cruciais para:
- Treinamento com batches de sequ√™ncias de diferentes comprimentos (padding mask)
- Prevenir "olhar para o futuro" em decoders (causal/autoregressive mask)

### Efficient Attention Mechanisms

- **Sparse Attention**: Limitam a aten√ß√£o a um subconjunto de tokens
- **Linear Attention**: Reduzem a complexidade computacional
- **Local Attention**: Focam em contextos locais
- **Longformer/Big Bird**: Combinam aten√ß√£o local com aten√ß√£o global seletiva

## üîó Casos de Uso

- [NLP Avan√ßado com Transformers](./use-case-advanced-nlp.md)
- [Vis√£o Computacional com Vision Transformers](./use-case-vision-transformers.md)

## üíª Frameworks e Bibliotecas

- **Hugging Face Transformers**: Biblioteca abrangente com implementa√ß√µes estado-da-arte
- **PyTorch**: Framework flex√≠vel para desenvolvimento e pesquisa
- **TensorFlow**: Alternativa robusta com alto n√≠vel de abstra√ß√£o
- **JAX**: Framework focado em diferencia√ß√£o autom√°tica e computa√ß√£o acelerada

## üöÄ Tend√™ncias e Desenvolvimentos Recentes

- **Scaling Laws**: Como o desempenho melhora com mais par√¢metros e dados
- **Sparse Expertise**: Modelos com trilh√µes de par√¢metros usando MoE (Mixture of Experts)
- **Efficient Transformers**: Arquiteturas que reduzem requisitos computacionais
- **Multimodal Transformers**: Integra√ß√£o de diferentes modalidades de dados
- **Specialized Transformers**: Adapta√ß√µes para dom√≠nios espec√≠ficos