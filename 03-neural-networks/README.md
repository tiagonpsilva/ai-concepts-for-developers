# ğŸ§  Redes Neurais

Redes Neurais Artificiais sÃ£o modelos computacionais inspirados na estrutura e funcionamento do cÃ©rebro humano, capazes de aprender padrÃµes complexos a partir de dados.

## ğŸ“‘ DefiniÃ§Ã£o

Uma rede neural Ã© composta por unidades de processamento interconectadas (neurÃ´nios artificiais) organizadas em camadas, que transformam inputs em outputs atravÃ©s de funÃ§Ãµes matemÃ¡ticas. O poder das redes neurais estÃ¡ em sua capacidade de aproximar praticamente qualquer funÃ§Ã£o matemÃ¡tica, permitindo modelar relaÃ§Ãµes altamente nÃ£o-lineares entre variÃ¡veis.

## ğŸ”„ Como Funcionam

```mermaid
graph LR
    A[Camada de Entrada] --> B[Camadas Ocultas]
    B --> C[Camada de SaÃ­da]
    
    subgraph "NeurÃ´nio"
        D[Entradas] --> E{Soma Ponderada}
        E --> F[FunÃ§Ã£o de AtivaÃ§Ã£o]
        F --> G[SaÃ­da]
    end
```

### Componentes BÃ¡sicos

#### O NeurÃ´nio Artificial

```mermaid
graph LR
    X1[xâ‚] --wâ‚--> Sum((Î£))
    X2[xâ‚‚] --wâ‚‚--> Sum
    X3[xâ‚ƒ] --wâ‚ƒ--> Sum
    B[1] --b--> Sum
    Sum --> F[f]
    F --> Y[y]
```

1. **Entradas (x)**: Dados ou sinais recebidos de outros neurÃ´nios
2. **Pesos (w)**: ParÃ¢metros que determinam a importÃ¢ncia relativa de cada entrada
3. **Bias (b)**: Termo constante que ajusta o limiar de ativaÃ§Ã£o
4. **FunÃ§Ã£o de Soma**: Calcula a soma ponderada das entradas (Î£ wÂ·x + b)
5. **FunÃ§Ã£o de AtivaÃ§Ã£o (f)**: Transforma a soma ponderada em um valor de saÃ­da

#### FunÃ§Ãµes de AtivaÃ§Ã£o Comuns

```mermaid
graph TD
    A[FunÃ§Ãµes de AtivaÃ§Ã£o] --> B[Sigmoid]
    A --> C[Tanh]
    A --> D[ReLU]
    A --> E[Leaky ReLU]
    A --> F[Softmax]
    
    B --> B1[Ïƒ(x) = 1/(1+e^-x)]
    C --> C1[tanh(x) = (e^x - e^-x)/(e^x + e^-x)]
    D --> D1[ReLU(x) = max(0, x)]
    E --> E1[Leaky ReLU(x) = max(Î±x, x)]
    F --> F1[Para classificaÃ§Ã£o multi-classe]
```

## ğŸ—ï¸ Arquiteturas de Redes Neurais

### Feedforward Neural Network (FNN)

```mermaid
graph LR
    I1[Input 1] --> H11[H1,1]
    I1 --> H12[H1,2]
    I1 --> H13[H1,3]
    
    I2[Input 2] --> H11
    I2 --> H12
    I2 --> H13
    
    I3[Input 3] --> H11
    I3 --> H12
    I3 --> H13
    
    H11 --> O1[Output 1]
    H12 --> O1
    H13 --> O1
    
    H11 --> O2[Output 2]
    H12 --> O2
    H13 --> O2
```

InformaÃ§Ã£o flui em uma direÃ§Ã£o (da entrada para a saÃ­da), sem ciclos ou loops.

### Redes Neurais Recorrentes (RNN)

```mermaid
graph LR
    X1[Xâ‚] --> A1[Aâ‚]
    A1 --> Y1[Yâ‚]
    A1 --> A2[Aâ‚‚]
    X2[Xâ‚‚] --> A2
    A2 --> Y2[Yâ‚‚]
    A2 --> A3[Aâ‚ƒ]
    X3[Xâ‚ƒ] --> A3
    A3 --> Y3[Yâ‚ƒ]
```

ContÃ©m ciclos internos de feedback, permitindo "memÃ³ria" para processar sequÃªncias.

### Redes Neurais Convolucionais (CNN)

```mermaid
graph LR
    A[Imagem de Entrada] --> B[Camadas Convolucionais]
    B --> C[Pooling]
    C --> D[Camadas Convolucionais]
    D --> E[Pooling]
    E --> F[Camadas Fully-Connected]
    F --> G[SaÃ­da]
```

Especializadas em processamento de dados estruturados em grade (imagens).

## ğŸ§® Treinamento de Redes Neurais

### Algoritmo de Backpropagation

```mermaid
graph TD
    A[Forward Pass] --> B[CÃ¡lculo do Erro]
    B --> C[Backward Pass]
    C --> D[AtualizaÃ§Ã£o de Pesos]
    D --> A
```

1. **Forward Pass**: Propaga a entrada atravÃ©s da rede para gerar uma previsÃ£o
2. **CÃ¡lculo do Erro**: Compara a previsÃ£o com o valor esperado
3. **Backward Pass**: Propaga o erro de volta, calculando gradientes
4. **AtualizaÃ§Ã£o de Pesos**: Ajusta os pesos usando otimizadores como SGD, Adam, etc.

### FunÃ§Ãµes de Perda Comuns

- **Mean Squared Error (MSE)**: Para problemas de regressÃ£o
- **Cross-Entropy**: Para problemas de classificaÃ§Ã£o
- **Categorical Cross-Entropy**: Para classificaÃ§Ã£o multi-classe
- **Binary Cross-Entropy**: Para classificaÃ§Ã£o binÃ¡ria

### Otimizadores

```mermaid
graph TD
    A[Otimizadores] --> B[Stochastic Gradient Descent SGD]
    A --> C[Adam]
    A --> D[RMSprop]
    A --> E[Adagrad]
    A --> F[Adadelta]
```

## ğŸ› ï¸ TÃ©cnicas AvanÃ§adas

### RegularizaÃ§Ã£o

```mermaid
graph TD
    A[RegularizaÃ§Ã£o] --> B[Dropout]
    A --> C[L1/L2 Regularization]
    A --> D[Batch Normalization]
    A --> E[Early Stopping]
    A --> F[Data Augmentation]
```

### InicializaÃ§Ã£o de Pesos

- **Xavier/Glorot**: MantÃ©m a variÃ¢ncia constante entre camadas
- **He**: Otimizada para funÃ§Ãµes de ativaÃ§Ã£o ReLU
- **Random Normal/Uniform**: InicializaÃ§Ã£o aleatÃ³ria simples

## ğŸ”— Casos de Uso

- [Sistema de RecomendaÃ§Ã£o com Redes Neurais](./use-case-recommendation-system.md)
- [DetecÃ§Ã£o de Anomalias em SÃ©ries Temporais](./use-case-anomaly-detection.md)

## ğŸ“š Recursos Adicionais

- Frameworks: TensorFlow, PyTorch, Keras
- Hardware: GPU, TPU para treinamento acelerado
- TÃ©cnicas de visualizaÃ§Ã£o: TensorBoard, Weights & Biases

## ğŸš€ TendÃªncias e Desafios

- **Redes Neurais ExplÃ­cÃ¡veis**: Entendendo as "caixas pretas"
- **Redes Neurais Neurais Eficientes**: Reduzindo necessidade computacional
- **Auto ML**: AutomaÃ§Ã£o da seleÃ§Ã£o e otimizaÃ§Ã£o de arquiteturas
- **Redes Neurais QuÃ¢nticas**: Aproveitando computaÃ§Ã£o quÃ¢ntica